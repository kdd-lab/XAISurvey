[
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "LRP",
            "long": "Layer-wise Relevance Propagation"
        },
        "reference": "Bach, S., Binder, A., Montavon, G., Klauschen, F., Mu ̈ller, K.-R., Samek, W.: On pixel-wise explanations for non-linear classifier decisions by layer- wise relevance propagation. PloS one 10(7), 0130140 (2015)",
        "authors": "Bach, S., Binder, A., Montavon, G., Klauschen, F., Mu ̈ller, K.-R., Samek, W.",
        "year": "2015",
        "title": "On pixel-wise explanations for non-linear classifier decisions by layer- wise relevance propagation.",
        "venue": "PloS one 10(7), 0130140",
        "cover_image": "",
        "link_to_code": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
        "description": ""
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "LIME",
            "long": "Local Interpretable Model-Agnostic Explanations"
        },
        "reference": "Ribeiro, M.T., Singh, S., Guestrin, C.: 'Why should i trust you?' explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 2016",
        "authors": "Ribeiro, M.T., Singh, S., Guestrin, C.",
        "year": "2016",
        "title": "Why should i trust you?",
        "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "cover_image": "lime_image.png",
        "link_to_code": "https://github.com/marcotcr/lime",
        "description": "Local explanations generated exploiting a random synthetic generation and Ridge regressors a surrogate model."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "SHAP",
            "long": "SHapley Additive exPlanations"
        },
        "reference": "Lundberg, S.M., Lee, S.-I.: A unified approach to interpreting model predictions. In: Advances in Neural Information Processing Systems, pp. 4765–4774 2017",
        "authors": "Lundberg, S.M., Lee, S.-I.",
        "year": "2017",
        "title": "A unified approach to interpreting model predictions.",
        "venue": "Advances in Neural Information Processing Systems",
        "cover_image": "shap_image.png",
        "link_to_code": "https://github.com/slundberg/shap",
        "description": "Local and global explanations generated exploiting a game theory approach."
    },
    {
        "type": "FI",
        "data_type": "TAB",
        "in_ph": "IN",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "EBM",
            "long": "Explainable Boosting Machine"
        },
        "reference": "Nori, H., Jenkins, S., Koch, P., Caruana, R.: Interpretml: A uni- fied framework for machine learning interpretability. arXiv preprint arXiv:1909.09223 2019",
        "authors": "Nori, H., Jenkins, S., Koch, P., Caruana, R.",
        "year": "2019",
        "title": "Interpretml: A uni- fied framework for machine learning interpretability.",
        "venue": "arXiv preprint",
        "cover_image": "",
        "link_to_code": "https://interpret.ml/docs/ebm.html",
        "description": "Interpretable by design model which outputs both local and global FI explanations."
    },
    {
        "type": "FI",
        "data_type": "TAB",
        "in_ph": "IN",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "NAM",
            "long": "Neural Additive Models"
        },
        "reference": "Agarwal, R., Frosst, N., Zhang, X., Caruana, R., Hinton, G.E.: Neural additive models: Interpretable machine learning with neural nets. arXiv preprint arXiv:2004.13912 2020",
        "authors": "Agarwal, R., Frosst, N., Zhang, X., Caruana, R., Hinton, G.E.",
        "year": "2020",
        "title": "Neural additive models: Interpretable machine learning with neural nets.",
        "venue": "arXiv preprint",
        "cover_image": "",
        "link_to_code": "https://github.com/nickfrosst/neural_additive_models",
        "description": "Local explainer tailored for neural networks."
    },
    {
        "type": "FI",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "CIU",
            "long": "Contextual Importance and Utility"
        },
        "reference": "Anjomshoae, S., Kampik, T., Fr ̈amling, K.: Py-ciu: A python library for explaining machine learning predictions using contextual importance and utility. In: IJCAI-PRICAI 2020 Workshop on Explainable Artificial Intelligence (XAI) (2020)",
        "authors": "Anjomshoae, S., Kampik, T., Fr ̈amling, K.",
        "year": "2020",
        "title": "Py-ciu: A python library for explaining machine learning predictions using contextual importance and utility.",
        "venue": "IJCAI-PRICAI 2020 Workshop on Explainable Artificial Intelligence (XAI)",
        "cover_image": "",
        "link_to_code": "https://github.com/TimKam/py-ciu",
        "description": "Local agnostic explainer tailored for taking into account the context of the features."
    },
    {
        "type": "FI",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "CoFrNets",
            "long": "Continued Fractions Nets"
        },
        "reference": "Zhou, Y., Hooker, G.: Interpreting models via single tree approximation. arXiv preprint arXiv:1610.09036 (2016)",
        "authors": "Zhou, Y., Hooker, G.",
        "year": "2016",
        "title": "Interpreting models via single tree approximation.",
        "venue": "arXiv preprint",
        "cover_image": "",
        "link_to_code": "",
        "description": "Local explainer tailored for neural networks."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "DALEX",
            "long": "DALEX"
        },
        "reference": "Biecek, P., Burzykowski, T.: Explanatory Model Analysis. Chapman and Hall/CRC, New York, (2021)",
        "authors": "Biecek, P., Burzykowski, T.",
        "year": "2021",
        "title": " Explanatory Model Analysis.",
        "venue": "Chapman and Hall/CRC",
        "cover_image": "",
        "link_to_code": "https://github.com/ModelOriented/DALEX",
        "description": "Local and agnostic explainer which exploits variable attribution."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "MSFT",
            "long": "MSFT"
        },
        "reference": "Chipman, H., George, E., McCulloh, R.: Making sense of a forest of trees. Computing Science and Statistics (1998)",
        "authors": "Chipman, H., George, E., McCulloh, R.",
        "year": "1998",
        "title": "Making sense of a forest of trees.",
        "venue": "Computing Science and Statistics",
        "cover_image": "",
        "link_to_code": "",
        "description": "Global, post-hoc, model-specific explainer for random forests that returns a decision trees."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "CMM",
            "long": "Combined Multiple Model procedure"
        },
        "reference": "Domingos, P.: Knowledge discovery via multiple models. Intelligent Data Analysis (1998)",
        "authors": "Domingos, P.",
        "year": "1998",
        "title": "Knowledge discovery via multiple models.",
        "venue": "Intelligent Data Analysis",
        "cover_image": "",
        "link_to_code": "",
        "description": "Global, post-hoc, model-specific explainer for models based on trees."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "DecText",
            "long": "Combined Multiple Model procedure"
        },
        "reference": "Boz, O.: Extracting decision trees from trained neural networks. In: Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2002)",
        "authors": "Boz, O.",
        "year": "2002",
        "title": "Extracting decision trees from trained neural networks.",
        "venue": "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
        "cover_image": "",
        "link_to_code": "",
        "description": "Global, post-hoc, model-specific explainer for neural networks, tailored to maximize model fidelity."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "IN",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "Scalable-Brl",
            "long": "Combined Multiple Model procedure"
        },
        "reference": "Yang, H., Rudin, C., Seltzer, M.: Scalable bayesian rule lists. In: International Conference on Machine Learning, pp. 3921–3930 (2017). PMLR",
        "authors": "Yang, H., Rudin, C., Seltzer, M.",
        "year": "2017",
        "title": "Scalable bayesian rule lists.",
        "venue": "International Conference on Machine Learning",
        "cover_image": "",
        "link_to_code": "",
        "description": "Interpretable rule-based model which exploits a Bayesian hierarchical model."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "LORE",
            "long": "LOcal Rule-based Explainer"
        },
        "reference": "Guidotti, R., Monreale, A., Giannotti, F., Pedreschi, D., Ruggieri, S., Turini, F.: Factual and counterfactual explanations for black box decision making. IEEE Intell. Syst. 34(6), 14–23 (2019)",
        "authors": "Guidotti, R., Monreale, A., Giannotti, F., Pedreschi, D., Ruggieri, S., Turini, F.",
        "year": "2019",
        "title": "Factual and counterfactual explanations for black box decision making.",
        "venue": "IEEE Intell. Syst.",
        "cover_image": "",
        "link_to_code": "https://github.com/riccotti/LORE",
        "description": "Local and agnostic model which outputs rules and counterfactual rules."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "RuleMatrix",
            "long": "LOcal Rule-based Explainer"
        },
        "reference": "Ming, Y., Qu, H., Bertini, E.: Rulematrix: Visualizing and understanding classifiers with rules. IEEE transactions on visualization and computer graphics 25(1), 342–352 (2018)",
        "authors": "Ming, Y., Qu, H., Bertini, E.",
        "year": "2018",
        "title": "Rulematrix: Visualizing and understanding classifiers with rules.",
        "venue": "IEEE transactions on visualization and computer graphics",
        "cover_image": "",
        "link_to_code": "https://github.com/rulematrix/rule-matrix-py",
        "description": "Local and global agnostic model which outputs rules."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "GlocalX",
            "long": "LOcal Rule-based Explainer"
        },
        "reference": "Setzu, M., Guidotti, R., Monreale, A., Turini, F.: Global explanations with local scoring. In: Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 159–171 (2019). Springer",
        "authors": "Setzu, M., Guidotti, R., Monreale, A., Turini, F.",
        "year": "2019",
        "title": "Global explanations with local scoring.",
        "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases",
        "cover_image": "",
        "link_to_code": "https://github.com/msetzu/glocalx",
        "description": "Global post-hox explainers which merges local explanations to provide a gloabl explanation."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "SkopeRules",
            "long": "SkopeRules"
        },
        "reference": "Friedman, J., Popescu, B.E.: Predictive learning via rule ensembles. The Annals of Applied Statistics 2, 916–954 (2008)",
        "authors": "Friedman, J., Popescu, B.E.",
        "year": "2008",
        "title": "Predictive learning via rule ensembles.",
        "venue": "The Annals of Applied Statistics",
        "cover_image": "",
        "link_to_code": "https://github.com/scikit-learn-contrib/skope-rules",
        "description": "SkopeRules defines an ensemble method and extracts rules from it."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "STA",
            "long": "Single Tree Approximation"
        },
        "reference": "Zhou, Y., Hooker, G.: Interpreting models via single tree approximation. arXiv preprint arXiv:1610.09036 (2016)",
        "authors": "Zhou, Y., Hooker, G.",
        "year": "2016",
        "title": "Interpreting models via single tree approximation.",
        "venue": "arXiv preprint",
        "cover_image": "",
        "link_to_code": "",
        "description": "Global, post-hoc, model-specific explainer for random forests."
    },
    
    {
        "type": "FI",
        "data_type": "TAB",
        "in_ph": "PH/IN",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "MAPLE",
            "long": "Model agnostic supervised local explanations"
        },
        "reference": "Plumb, G., Molitor, D., Talwalkar, A.S.: Model agnostic supervised local explanations. In: Advances in Neural Information Processing Systems (2018)",
        "authors": "Plumb, G., Molitor, D., Talwalkar, A.S",
        "year": "2018",
        "title": "Model agnostic supervised local explanations",
        "venue": "Advances in Neural Information Processing Systems",
        "cover_image": "",
        "link_to_code": "https://github.com/GDPlumb/MAPLE",
        "description": "MAPLE is a local post-hoc model-agnostic explainer that can also be used as a transparent model due to its internal structure."
    },
    {
        "type": "RB",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "TREPAN",
            "long": "Extracting tree-structured representations of trained networks"
        },
        "reference": "",
        "authors": "Craven, M., Shavlik, J.W",
        "year": "1996",
        "title": "Extracting tree-structured representations of trained networks",
        "venue": "Advances in Neural Information Processing Systems",
        "cover_image": "",
        "link_to_code": "https://github.com/abarthakur/trepan_python",
        "description": ""
    },
    {
        "type": "RB",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "G/L",
        "agn_spec": "A",
        "name": {
            "short": "ANCHOR",
            "long": "Anchors: High-precision model- agnostic explanations"
        },
        "reference": "",
        "authors": "Ribeiro, M.T., Singh, S., Guestrin, C.",
        "year": "2018",
        "title": "Anchors: High-precision model- agnostic explanations",
        "venue": "AAAI",
        "cover_image": "anchors_image.png",
        "link_to_code": "https://github.com/marcotcr/anchor",
        "description": ""
    },
    {
        "type": "PR",
        "data_type": "TAB",
        "in_ph": "IN",
        "glo_loc": "G/L",
        "agn_spec": "S",
        "name": {
            "short": "PS",
            "long": "Prototype Selection"
        },
        "reference": "Bien, J., Tibshirani, R.: Prototype selection for interpretable classification. The Annals of Applied Statistics, 2403–2424 (2011)",
        "authors": "Bien, J., Tibshirani, R.",
        "year": "2011",
        "title": "Prototype selection for interpretable classification.",
        "venue": "The Annals of Applied Statistics",
        "cover_image": "",
        "link_to_code": "",
        "description": "It exploits a cover optimization process to find the set that best represents the data under analysis."
    },
    {
        "type": "PR",
        "data_type": "ANY",
        "in_ph": "IN",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "MMD-critic",
            "long": "Prototype Selection"
        },
        "reference": "Kim, B., Khanna, R., Koyejo, O.O.: Examples are not enough, learn to criticize! Criticism for interpretability. In: Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 29. Curran Associates, Inc., (2016). https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534- Paper.pdf",
        "authors": "Kim, B., Khanna, R., Koyejo, O.O.",
        "year": "2016",
        "title": "Examples are not enough, learn to criticize! Criticism for interpretability.",
        "venue": "Advances in Neural Information Processing Systems",
        "cover_image": "",
        "link_to_code": "https://github.com/BeenKim/MMD-critic",
        "description": "It extracts from the data both prototypes and criticisms, which are representations of the dataset far from the common behaviour."
    },
    {
        "type": "PR",
        "data_type": "ANY",
        "in_ph": "IN",
        "glo_loc": "G",
        "agn_spec": "A",
        "name": {
            "short": "ProtoDash",
            "long": "ProtoDash"
        },
        "reference": "Gurumoorthy, K.S., Dhurandhar, A., Cecchi, G., Aggarwal, C.: Efficient data representation by selecting prototypes with importance weights. In: 2019 IEEE International Conference on Data Mining (ICDM), pp. 260–269 (2019). IEEE",
        "authors": "Gurumoorthy, K.S., Dhurandhar, A., Cecchi, G., Aggarwal, C.",
        "year": "2019",
        "title": "Efficient data representation by selecting prototypes with importance weights.",
        "venue": "IEEE International Conference on Data Mining (ICDM)",
        "cover_image": "",
        "link_to_code": "https://github.com/Trusted-AI/AIX360",
        "description": "Global explainer which outputs prototypes with a weight associated."
    },
    {
        "type": "CF",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "CEM",
            "long": "Contrastive Explanations Method"
        },
        "reference": "Dhurandhar, A., Chen, P.-Y., Luss, R., Tu, C.-C., Ting, P., Shanmugam, K., Das, P.: Explanations based on the missing: Towards contrastive explanations with pertinent negatives. In: Advances in Neural Information Processing Systems (2018)",
        "authors": "Dhurandhar, A., Chen, P.-Y., Luss, R., Tu, C.-C., Ting, P., Shanmugam, K., Das, P.",
        "year": "2018",
        "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives.",
        "venue": "Advances in Neural Information Processing Systems",
        "cover_image": "",
        "link_to_code": "https://github.com/IBM/Contrastive-Explanation-Method",
        "description": "It outputs counterfactulas which are minimal and necessary."
    },
    {
        "type": "CF",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "CFX",
            "long": "CFX"
        },
        "reference": "Albini, E., Rago, A., Baroni, P., Toni, F.: Relation-based counterfac- tual explanations for bayesian network classifiers. In: IJCAI, pp. 451–457 (2020)",
        "authors": "Albini, E., Rago, A., Baroni, P., Toni, F.",
        "year": "2020",
        "title": "Relation-based counterfac- tual explanations for bayesian network classifiers.",
        "venue": "IJCAI",
        "cover_image": "",
        "link_to_code": "",
        "description": "It is a model-specific post-hoc explainer for Bayesian Network Classifiers"
    },
    {
        "type": "CF",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "DICE",
            "long": "Diverse Counterfactual Explanations"
        },
        "reference": "Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classifiers through diverse counterfactual explanations. In: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (2020)",
        "authors": "Mothilal, R.K., Sharma, A., Tan, C.",
        "year": "2020",
        "title": "Explaining machine learning classfiers through diverse counterfactual explanations.",
        "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",
        "cover_image": "",
        "link_to_code": "https://github.com/interpretml/DiCE",
        "description": "It is a model-agnostic post-hoc explainer which outputs counterfactuals that are plausible and diverse."
    },
    {
        "type": "CF",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "C-CHVAE",
            "long": ""
        },
        "reference": "Pawelczyk, M., Broelemann, K., Kasneci, G.: Learning model-agnostic counterfactual explanations for tabular data. In: Proceedings of The Web Conference 2020. WWW ’20 (2020)",
        "authors": "Pawelczyk, M., Broelemann, K., Kasneci, G.",
        "year": "2020",
        "title": "Learning model-agnostic counterfactual explanations for tabular data.",
        "venue": "Proceedings of The Web Conference 2020",
        "cover_image": "",
        "link_to_code": "https://github.com/MartinPawel/c-chvae",
        "description": "It is a model-agnostic post-hoc explainer which outputs counterfactuals that are plausible and diverse."
    },
    {
        "type": "CF",
        "data_type": "TAB",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "FACE",
            "long": "Feasible and Actionable counterfactual explanations"
        },
        "reference": "Poyiadzi, R., Sokol, K., Santos-Rodriguez, R., De Bie, T., Flach, P.: Face: feasible and actionable counterfactual explanations. In: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (2020)",
        "authors": "Poyiadzi, R., Sokol, K., Santos-Rodriguez, R., De Bie, T., Flach, P.",
        "year": "2020",
        "title": "Face: feasible and actionable counterfactual explanations.",
        "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",
        "cover_image": "",
        "link_to_code": "https://github.com/MartinPawel/c-chvae",
        "description": "It is a model-agnostic post-hoc explainer which outputs counterfactuals that are feasible and actionable."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "epsilon-LRP",
            "long": "epsilon Layer-wise Relevance Propagation"
        },
        "reference": "",
        "authors": "Bach, S., Binder, A., Montavon, G., Klauschen, F., Mu ̈ller, K.-R., Samek, W",
        "year": "2015",
        "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
        "venue": "plos journal",
        "cover_image": "lrp_image.png",
        "link_to_code": "https://github.com/marcoancona/DeepExplain",
        "description": "Layer-wise Relevance Propagation was introduced for feed-forward neural networks and then adapted to different type of models. It decomposes the prediction y backward using local redistribution rules until it assigns a relevance score R to each pixel value."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "LIME",
            "long": "Local Interpretable Model-agnostic Explanations"
        },
        "reference": "",
        "authors": "Ribeiro, M.T., Singh, S., Guestrin, C.:",
        "year": "2016",
        "title": "Why should i trust you? explaining the predictions of any classifier",
        "venue": "KDD",
        "cover_image": "",
        "link_to_code": "https://github.com/marcotcr/lime",
        "description": "Intuitively, an explanation is a local linear approximation of the model's behaviour. While the model may be very complex globally, it is easier to approximate it around the vicinity of a particular instance. While treating the model as a black box, we perturb the instance we want to explain and learn a sparse linear model around it, as an explanation."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "SHAP",
            "long": "SHapley Additive exPlanations"
        },
        "reference": "",
        "authors": "Lundberg, S.M., Lee, S.-I",
        "year": "2016",
        "title": "A unified approach to interpreting model predictions",
        "venue": "NeurIPS",
        "cover_image": "",
        "link_to_code": "https://github.com/slundberg/shap",
        "description": "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "Grad-CAM",
            "long": "Gradient-weighted Class Activation Mapping"
        },
        "reference": "",
        "authors": "Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.",
        "year": "2017",
        "title": "Grad-cam: Visual explanations from deep networks via gradient- based localization",
        "venue": "IEEE CVPR",
        "cover_image": "grad-cam_image.png",
        "link_to_code": "https://github.com/adityac94/Grad_CAM_plus_plus",
        "description": "(Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept"
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "DeepLIFT",
            "long": "Deep Learning Important FeaTures"
        },
        "reference": "",
        "authors": "Shrikumar, A., Greenside, P., Kundaje, A",
        "year": "2017",
        "title": "Learning important features through propagating activation differences",
        "venue": "ICML",
        "cover_image": "deeplift_image.png",
        "link_to_code": "https://github.com/marcoancona/DeepExplain",
        "description": "DeepLIFT recognizes that what we care about is not the gradient, which describes how y changes as x changes at the point x, but the slope, which describes how y changes as x differs from the baseline."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "Smoothgrad",
            "long": "Smoothed Gradients saliency maps"
        },
        "reference": "",
        "authors": "Smilkov, D., Thorat, N., Kim, B., Vi ́egas, F., Wattenberg, M.",
        "year": "2017",
        "title": "Smooth-grad: removing noise by adding noise.",
        "venue": "",
        "cover_image": "smoothgrad_image.png",
        "link_to_code": "https://github.com/PAIR-code/saliency",
        "description": "Usually, a SM is created directly on the gradient of the black-box output w.r.t. the input. Smoothgrad augments this process by smoothing the gradients with a Gaussian noise kernel. More in detail, it takes the image x, applies Gaussian noise to it, and retrieves the saliency map for every perturbed image, using the gradient. The final saliency map is an average of these perturbed SMs."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "INTGRAD",
            "long": "Integrated Gradients"
        },
        "reference": "",
        "authors": "Sundararajan, M., Taly, A., Yan, Q.",
        "year": "2017",
        "title": "Axiomatic attribution for deep network",
        "venue": "ICML",
        "cover_image": "intgrad_image.png",
        "link_to_code": "https://captum.ai/docs/extension/integrated_gradients",
        "description": "Integrated gradients is a simple, yet powerful axiomatic attribution method that uses gradients of the model to produce feature importance."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "GradCAM++",
            "long": "Gradient-weighted Class Activation Mapping Plus Plus"
        },
        "reference": "",
        "authors": "Chattopadhay, A., Sarkar, A., Howlader, P., Balasubramanian, V.N.",
        "year": "2018",
        "title": "Grad-cam++: Generalized gradient-based visual explanations for deep convolutional network",
        "venue": "WACV",
        "cover_image": "gradcam++_image.png",
        "link_to_code": "https://github.com/adityac94/Grad_CAM_plus_plus",
        "description": "An improved version of Grad-CAM where also higher order derivates are considered when creating the explanation."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "RISE",
            "long": "Randomized Input Sampling for Explanation"
        },
        "reference": "",
        "authors": "Petsiuk, V., Das, A., Saenko, K",
        "year": "2018",
        "title": "Rise: Randomized input sampling for explanation of black-box models",
        "venue": "BMVC",
        "cover_image": "rise_image.png",
        "link_to_code": "https://github.com/eclique/RISE",
        "description": "To generate a saliency map for model's prediction, RISE queries black-box model on multiple randomly masked versions of input. After all the queries are done we average all the masks with respect to their scores to produce the final saliency map. The idea behind this is that whenever a mask preserves important parts of the image it gets higher score, and consequently has a higher weight in the sum."
    },
    {
        "type": "FI",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "ANCHOR",
            "long": "Anchor"
        },
        "reference": "",
        "authors": "Ribeiro, M.T., Singh, S., Guestrin, C",
        "year": "2018",
        "title": "Anchors: High-precision model-agnostic explanations.",
        "venue": "AAAI",
        "cover_image": "",
        "link_to_code": "https://github.com/marcotcr/anchor",
        "description": "An anchor explanation is a rule that sufficiently “anchors” the prediction locally such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same. The anchor method is able to explain any black box classifier, with two or more classes. All it is required is that the classifier implements a function that takes in raw text or a numpy array and outputs a prediction (integer)."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "EXT-PERT",
            "long": "Extreme Perturbation"
        },
        "reference": "",
        "authors": "Fong, R., Patrick, M., Vedaldi, A",
        "year": "2019",
        "title": "Understanding deep networks via extremal perturbations and smooth masks.",
        "venue": "CVPR",
        "cover_image": "ext_pert_image.png",
        "link_to_code": "https://github.com/facebookresearch/TorchRay",
        "description": "Extremal perturbations are regions of an image that, for a given area (boxed), maximally affect the activation of a certain neuron in a neural network (i.e., “mousetrap” class score). As the area of the perturbation is increased, the method reveals more of the image, in order of decreasing importance"
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "XRAI",
            "long": "Xrai: Better attributions through regions"
        },
        "reference": "",
        "authors": "Kapishnikov, A., Bolukbasi, T., Viegas, F., Terry, M.",
        "year": "2019",
        "title": "Xrai: Better attributions through regions.",
        "venue": "ICCV",
        "cover_image": "XRAI_image.png",
        "link_to_code": "https://github.com/PAIR-code/saliency",
        "description": "XRAI add segmentation to INTGRAD explanations. XRAI first over-segments the image, then it iteratively tests each region's importance, fusing smaller regions into larger segments based on attribution scores obtained using INTGRAD."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "CXPlain",
            "long": "Causal eXPlanation"
        },
        "reference": "",
        "authors": "Schwab, P., Karlen, W.",
        "year": "2019",
        "title": "Cxplain: Causal explanations for model interpretation under uncertainty",
        "venue": "NeurIPS",
        "cover_image": "cxplain_image.png",
        "link_to_code": "https://github.com/d909b/cxplain",
        "description": "Causal Explanations (CXPlain) is a method for explaining the decisions of any machine-learning model. CXPlain uses explanation models trained with a causal objective to learn to explain machine-learning models, and to quantify the uncertainty of its explanations."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "Eigen-CAM",
            "long": "principal components based Class Activation Maps"
        },
        "reference": "",
        "authors": "Muhammad, M.B., Yeasin, M.",
        "year": "2020",
        "title": "Eigen-cam: Class activation map using principal components",
        "venue": "IJCNN",
        "cover_image": "",
        "link_to_code": "https://github.com/shyhyawJou/Eigen-CAM-Tensorflow",
        "description": "Eigen-CAM is an approach that provides a simpler and intuitive way of generating CAM. Eigen-CAM computes and visualizes the principle components of the learned features/representations from the convolutional layers to generate saliency maps."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "Ablation-CAM",
            "long": "Ablation-based Class Activation Mapping"
        },
        "reference": "",
        "authors": "Ramaswamy, Harish Guruprasad.",
        "year": "2020",
        "title": "Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization",
        "venue": "IEEE/CVF",
        "cover_image": "",
        "link_to_code": "https://github.com/shyhyawJou/Ablation-CAM-Tensorflow",
        "description": "Ablation-based Class Activation Mapping (Ablation CAM) uses ablation analysis to determine the importance (weights) of individual feature map units w.r.t. class. Further, this is used to produce a coarse localization map highlighting the important regions in the image for predicting the concept."
    },
    {
        "type": "FI",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "S",
        "name": {
            "short": "Score-CAM",
            "long": "Score-Weighted Class Activation Mapping"
        },
        "reference": "",
        "authors": "Wang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel, P., Hu, X.",
        "year": "2020",
        "title": "Score-cam: Score-weighted visual explanations for convolu- tional neural networks",
        "venue": "IEEE/CVF",
        "cover_image": "scorecam_image.png",
        "link_to_code": "https://github.com/haofanwang/Score-CAM",
        "description": "Unlike others class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class. The final result is obtained by a linear combination of weights and activation maps."
    },
    {
        "type": "CA",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "TCAV",
            "long": "Testing with Concept Attribution Vectors"
        },
        "reference": "",
        "authors": "Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F.,",
        "year": "2018",
        "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors",
        "venue": "ICML",
        "cover_image": "tcav_image.png",
        "link_to_code": "https://github.com/tensorflow/tcav",
        "description": "Testing with Concept Activation Vectors (TCAV) is a new interpretability method to understand what signals your neural networks models uses for prediction. Typical interpretability methods show importance weights in each input feature (e.g, pixel). TCAV instead shows importance of high level concepts (e.g., color, gender, race) for a prediction class."
    },
    {
        "type": "CA",
        "data_type": "IMG",
        "in_ph": "IN",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "ICNN",
            "long": "Interpretable Convolutional Neural Networks"
        },
        "reference": "",
        "authors": "Zhang, Q., Wu, Y.N., Zhu, S.-C",
        "year": "2018",
        "title": "Interpretable convolutional neural networks.",
        "venue": "CVPR",
        "cover_image": "retain_image.png",
        "link_to_code": "https://github.com/ada-shen/ICNN",
        "description": "Retain is a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high convolutional layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process."
    },

    {
        "type": "CA",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "A",
        "name": {
            "short": "ACE",
            "long": "Automatic Concept-based Explanations."
        },
        "reference": "",
        "authors": "Ghorbani, A., Wexler, J., Zou, J.Y., Kim, B.",
        "year": "2019",
        "title": "Towards automatic concept-based explanations.",
        "venue": "NeurIPS",
        "cover_image": "ace_image.png",
        "link_to_code": "https://github.com/amiratag/ACE",
        "description": "ACE is a modification of TCAV in which concepts are automatically discovered using segmentation and clustering instead of using a set of pre-selected images."
    },


    {
        "type": "CA",
        "data_type": "IMG",
        "in_ph": "IN",
        "glo_loc": "G",
        "agn_spec": "A",
        "name": {
            "short": "CaCE",
            "long": "Causal Concept Effect"
        },
        "reference": "",
        "authors": "Goyal, Y., Feder, A., Shalit, U., Kim, B.",
        "year": "2019",
        "title": "Explaining classifiers with causal concept effect",
        "venue": "",
        "cover_image": "",
        "link_to_code": "",
        "description": "Causal Concept Effect is another variation of TCAV that looks at the causal effect of the presence or absence of high-level concepts on the black-box prediction."
    },

    {
        "type": "CA",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "A",
        "name": {
            "short": "ConceptSHAP",
            "long": "Concept-based SHAPley values"
        },
        "reference": "",
        "authors": "Yeh, C.-K., Kim, B., Arik, S., Li, C.-L., Pfister, T., Ravikumar, P.",
        "year": "2020",
        "title": "On completeness-aware concept-based explanations in deep neural networks.",
        "venue": "NeurIPS",
        "cover_image": "Concept_SHAP.png",
        "link_to_code": "https://github.com/chihkuanyeh/concept_exp",
        "description": "ConceptSHAP is an evolution of SHAP which tries to define an importance score for each concept discovered."
    },
    {
        "type": "CA",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "PACE",
            "long": "Posthoc Architecture-agnostic Concept Extractor"
        },
        "reference": "",
        "authors": "Kamakshi, V., Gupta, U., Krishnan, N.C.",
        "year": "2021",
        "title": "Pace: Posthoc architecture-agnostic concept extractor for explaining cnns",
        "venue": "IJCNN",
        "cover_image": "pace_image.png",
        "link_to_code": "",
        "description": "PACE automatically extracts smaller sub-regions of the image called concepts relevant to the black-box prediction. It tightly integrates the faithfulness of the explanatory framework to the black-box model."
    },
    {
        "type": "CF",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "L2X",
            "long": "Learning to Explain"
        },
        "reference": "",
        "authors": "Chen, J., Song, L., Wainwright, M.J., Jordan, M.I.",
        "year": "2018",
        "title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation",
        "venue": "ICML",
        "cover_image": "l2x_image.png",
        "link_to_code": "https://github.com/Jianbo-Lab/L2X",
        "description": "L2X is based on learning a function for extracting a subset of the most informative features for each given sample using Mutual Information."
    },
    {
        "type": "CF",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "CEM",
            "long": "Contrastive Explanation Method"
        },
        "reference": "",
        "authors": "Dhurandhar, A., Chen, P.-Y., Luss, R., Tu, C.-C., Ting, P., Shanmugam, K., Das, P.",
        "year": "2018",
        "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives",
        "venue": "NeurIPS",
        "cover_image": "cem_image.png",
        "link_to_code": "https://docs.seldon.io/projects/alibi/en/stable/methods/CEM.html",
        "description": "CEM generates instance based local black box explanations for classification models in terms of Pertinent Positives (PP) and Pertinent Negatives (PN). For a PP, the method finds the features that should be minimally and sufficiently present (e.g. important pixels in an image) to predict the same class as on the original instance. PN’s on the other hand identify what features should be minimally and necessarily absent from the instance to be explained in order to maintain the original prediction class."
    },
    {
        "type": "CF",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "CFProto",
            "long": "CounterFactuals guided by Prototypes"
        },
        "reference": "",
        "authors": "Van Looveren, A., Klaise, J",
        "year": "2019",
        "title": "Interpretable counterfactual explanations guided by prototypes",
        "venue": "ECML/PKDD",
        "cover_image": "CFProto_image.png",
        "link_to_code": "https://docs.seldon.io/projects/alibi/en/stable/methods/CFProto.html",
        "description": "CFProto proposes a fast, model agnostic method to find interpretable counterfactual explanations for classifier predictions by using class prototypes."
    },
    {
        "type": "CF",
        "data_type": "IMG",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "ABELE",
            "long": "Adversarial Black box Explainer generating Latent Exemplars"
        },
        "reference": "",
        "authors": "Guidotti, R., Monreale, A., Matwin, S., Pedreschi, D",
        "year": "2020",
        "title": "Explaining image classifiers generating exemplars and counter-exemplars from latent representations.",
        "venue": "AAAI",
        "cover_image": "abele_image.png",
        "link_to_code": "https://github.com/riccotti/ABELE",
        "description": "ABELE is a local, model-agnostic explanation method able to overcome the existing limitations of the local approaches by exploiting the latent feature space, learned through an adversarial autoencoder, for the neighborhood generation process. Given an image classified by a given black box model, ABELE provides an explanation for the reasons of the proposed classification. The explanation consists of two parts: (i) a set of exemplars and counter-exemplars images illustrating, respectively, instances classified with the same label and with a different label than the instance to explain, which may be visually analyzed to understand the reasons for the classification, and (ii) a saliency map highlighting the areas of the image to explain that contribute to its classification, and areas of the image that push it towards another label."
    },
    {
        "type": "PR",
        "data_type": "ANY",
        "in_ph": "IN",
        "glo_loc": "G",
        "agn_spec": "A",
        "name": {
            "short": "MMD-critic",
            "long": "statistical model criticisms using the Maximum Mean Discrepancy"
        },
        "reference": "",
        "authors": "Kim, B., Khanna, R., Koyejo, O.O.",
        "year": "2016",
        "title": "Examples are not enough, learn to criticize! criticism for interpretability",
        "venue": "NeurIPS",
        "cover_image": "mmdcritic_image.png",
        "link_to_code": "https://github.com/BeenKim/MMD-critic",
        "description": "Motivated by the Bayesian model criticism framework, MMD-critic efficiently learns prototypes and criticism, designed to aid human interpretability."
    },
    {
        "type": "PR",
        "data_type": "ANY",
        "in_ph": "PH",
        "glo_loc": "L",
        "agn_spec": "A",
        "name": {
            "short": "Influence Functions",
            "long": "Influence Functions"
        },
        "reference": "",
        "authors": "Koh, P.W., Liang, P",
        "year": "2017",
        "title": "Understanding black-box predictions via influence functions.",
        "venue": "ICML",
        "cover_image": "InfluenceFunction_image.png",
        "link_to_code": "https://github.com/kohpangwei/influence-release",
        "description": "Influence functions are a classic technique from robust statistics to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction."
    },
    {
        "type": "PR",
        "data_type": "IMG",
        "in_ph": "IN",
        "glo_loc": "G",
        "agn_spec": "S",
        "name": {
            "short": "ProtoPNet",
            "long": "Prototypical Part Network"
        },
        "reference": "",
        "authors": "Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.",
        "year": "2019",
        "title": "This looks like that: deep learning for interpretable image recognition",
        "venue": "NeurIPS",
        "cover_image": "protopnet_image.png",
        "link_to_code": "https://github.com/cfchen-duke/ProtoPNet",
        "description": "When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. ProtoPNet reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification."
    }
]